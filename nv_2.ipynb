{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\", # Bắt buộc kết nối qua mạng TCP/IP\n",
    "    dbname=\"postgres\", \n",
    "    user=\"postgres\", \n",
    "    password=\"postgres_password\"\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b37a8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cảnh báo: Không thể chuyển model sang GPU: CUDA error: device-side assert triggered\n",
      "Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      " Câu hỏi: Trong mô hình nhà thông minh, IoT chủ yếu đóng vai trò gì?\n",
      "  A. Lưu trữ dữ liệu trên máy chủ  →  distance = 9.8515\n",
      "  B. Kết nối Internet và quản lý thiết bị từ xa  →  distance = 9.5775\n",
      "  C. Cung cấp dịch vụ phân tích dữ liệu lớn  →  distance = 10.6536\n",
      "  D. Thay thế hoàn toàn điện toán đám mây  →  distance = 9.8783\n",
      "👉 Đáp án gợi ý: B - Kết nối Internet và quản lý thiết bị từ xa\n",
      "\n",
      " Câu hỏi: Năm 2010, công nghệ nào thường được sử dụng trong kiểm soát ra vào ngôi nhà thông minh?\n",
      "  A. Bluetooth Low Energy  →  distance = 10.9916\n",
      "  B. RFID và nhận dạng khuôn mặt/vân tay  →  distance = 9.3244\n",
      "  C. Zigbee  →  distance = 10.1951\n",
      "  D. Blockchain  →  distance = 10.1562\n",
      "👉 Đáp án gợi ý: B - RFID và nhận dạng khuôn mặt/vân tay\n",
      "\n",
      " Câu hỏi: Bộ cảm biến trong nhà thông minh thực hiện chức năng gì và bộ truyền động có vai trò gì?\n",
      "  A. Bộ cảm biến hiển thị dữ liệu, bộ truyền động thu thập dữ liệu  →  distance = 8.3637\n",
      "  B. Bộ cảm biến lưu trữ dữ liệu, bộ truyền động hiển thị dữ liệu  →  distance = 8.3250\n",
      "  C. Bộ cảm biến thu thập dữ liệu, bộ truyền động thực hiện lệnh từ máy chủ hoặc thiết bị điều khiển  →  distance = 8.4127\n",
      "  D. Bộ cảm biến điều khiển thiết bị, bộ truyền động lưu trữ dữ liệu  →  distance = 8.3850\n",
      "👉 Đáp án gợi ý: B - Bộ cảm biến lưu trữ dữ liệu, bộ truyền động hiển thị dữ liệu\n",
      "\n",
      " Câu hỏi: Nội dung chính của bài báo nghiên cứu Public_001 về IoT trong xây dựng nhà thông minh là gì?\n",
      "  A. Giới thiệu các ứng dụng cụ thể của điện toán đám mây  →  distance = 10.9261\n",
      "  B. Phân tích lợi ích và chi phí triển khai hệ thống cảm biến  →  distance = 11.4108\n",
      "  C. Phân loại và đánh giá nghiên cứu IoT ứng dụng nhà thông minh trong 10 năm qua  →  distance = 11.4527\n",
      "  D. Trình bày chi tiết về công nghệ vi mạch trong IoT  →  distance = 11.5481\n",
      "👉 Đáp án gợi ý: A - Giới thiệu các ứng dụng cụ thể của điện toán đám mây\n",
      "\n",
      " Câu hỏi: Dựa vào thông tin trong tài liệu Public_001, hãy tính số bài báo trung bình mỗi lớp trong nghiên cứu được đề cập.\n",
      "  A. 45-50  →  distance = 10.3552\n",
      "  B. 55-60  →  distance = 10.3300\n",
      "  C. 70-80  →  distance = 11.9169\n",
      "  D. 90-100  →  distance = 10.1353\n",
      "👉 Đáp án gợi ý: D - 90-100\n",
      "\n",
      " Câu hỏi: Theo tài liệu Public_002, học phần Bảo mật Web trang bị kiến thức về bộ rủi ro phổ biến nào?\n",
      "  A. OWASP Top 10  →  distance = 9.0374\n",
      "  B. IEEE 802.11  →  distance = 9.4525\n",
      "  C. ISO/IEC 27001  →  distance = 8.8918\n",
      "  D. NIST Cybersecurity Framework  →  distance = 8.8534\n",
      "👉 Đáp án gợi ý: D - NIST Cybersecurity Framework\n",
      "\n",
      " Câu hỏi: Dựa trên tài liệu Public_002, mô tả học phần Cơ sở dữ liệu nâng cao, hãy liệt kê các loại CSDL không được đề cập trong nội dung học phần này.\n",
      "  A. CSDL NoSQL  →  distance = 10.0470\n",
      "  B. CSDL hướng đối tượng  →  distance = 9.6025\n",
      "  C. CSDL bán cấu trúc XML  →  distance = 9.6373\n",
      "  D. CSDL quan hệ truyền thống  →  distance = 9.5590\n",
      "👉 Đáp án gợi ý: D - CSDL quan hệ truyền thống\n",
      "\n",
      " Câu hỏi: Nêu chi tiết nội dung đào tạo về bảo mật web và bảo mật cơ sở dữ liệu trong tài liệu Public_002.\n",
      "  A. OWASP Top 10 / CSDL NoSQL  →  distance = 10.8811\n",
      "  B. IEEE 802.11 / CSDL hướng đối tượng  →  distance = 10.9088\n",
      "  C. DSA / CSDL bán cấu trúc XML  →  distance = 10.9699\n",
      "  D. Toán cao cấp / CSDL quan hệ truyền thống  →  distance = 11.4299\n",
      "👉 Đáp án gợi ý: A - OWASP Top 10 / CSDL NoSQL\n",
      "\n",
      " Câu hỏi: Nhóm môn học trong tài liệu Public_002 tập trung cung cấp cho sinh viên kiến thức tổng quan về lĩnh vực nào?\n",
      "  A. Cơ khí – Điện tử  →  distance = 9.6892\n",
      "  B. Công nghệ thông tin (CSDL, mạng, bảo mật, dữ liệu lớn, IoT, học máy)  →  distance = 7.0313\n",
      "  C. Kinh tế – Quản lý  →  distance = 9.4850\n",
      "  D. Ngôn ngữ học  →  distance = 9.7276\n",
      "👉 Đáp án gợi ý: B - Công nghệ thông tin (CSDL, mạng, bảo mật, dữ liệu lớn, IoT, học máy)\n",
      "\n",
      " Câu hỏi: Trong tài liệu Public_002, đối với đoạn liệt kê các học phần từ số 47 đến 81, nếu các học phần này được trải đều trên 7 trang, trung bình mỗi trang mô tả bao nhiêu học phần?\n",
      "  A. 5  →  distance = 10.3733\n",
      "  B. 6  →  distance = 10.5861\n",
      "  C. 7  →  distance = 10.0191\n",
      "  D. 8  →  distance = 10.0928\n",
      "👉 Đáp án gợi ý: C - 7\n",
      "\n",
      " Câu hỏi: Điểm BLEU được dùng để đánh giá chất lượng bản dịch trong tài liệu Public_003 là gì?\n",
      "  A. Một thuật toán dịch máy  →  distance = 10.2217\n",
      "  B. Một loại dữ liệu song ngữ  →  distance = 9.9553\n",
      "  C. Một phương pháp đánh giá tự động chất lượng dịch máy  →  distance = 10.1526\n",
      "  D. Một hệ dịch mã nguồn mở  →  distance = 10.3071\n",
      "👉 Đáp án gợi ý: B - Một loại dữ liệu song ngữ\n",
      "\n",
      " Câu hỏi: Công cụ nào được nhóm tác giả của tài liệu Public_003 sử dụng để huấn luyện các hệ dịch nơ ron?\n",
      "  A. Moses  →  distance = 9.7865\n",
      "  B. OpenNMT  →  distance = 11.0836\n",
      "  C. TensorFlow  →  distance = 9.3547\n",
      "  D. Fairseq  →  distance = 9.2159\n",
      "👉 Đáp án gợi ý: D - Fairseq\n",
      "\n",
      " Câu hỏi: Điểm BLEU và công cụ huấn luyện hệ dịch nơ ron trong nghiên cứu này là gì?\n",
      "  A. Một thuật toán dịch máy / Moses  →  distance = 9.9506\n",
      "  B. Một loại dữ liệu song ngữ / OpenNMT  →  distance = 9.8316\n",
      "  C. Một phương pháp đánh giá tự động chất lượng dịch máy / TensorFlow  →  distance = 9.2654\n",
      "  D. Một hệ dịch mã nguồn mở / Fairseq  →  distance = 9.5456\n",
      "👉 Đáp án gợi ý: C - Một phương pháp đánh giá tự động chất lượng dịch máy / TensorFlow\n",
      "\n",
      " Câu hỏi: Nội dung chính của tài liệu Public_003 là gì?\n",
      "  A. So sánh dịch máy thống kê và dịch máy ví dụ  →  distance = 10.1350\n",
      "  B. Đề xuất phương pháp thích ứng miền mới cho dịch máy nơ ron Anh – Việt, tận dụng dữ liệu đơn ngữ ngoài miền để cải thiện chất lượng dịch  →  distance = 9.9033\n",
      "  C. Phân tích dữ liệu song ngữ trong lĩnh vực pháp lý  →  distance = 9.9673\n",
      "  D. Trình bày lý thuyết về RNN hai chiều  →  distance = 10.0892\n",
      "👉 Đáp án gợi ý: B - Đề xuất phương pháp thích ứng miền mới cho dịch máy nơ ron Anh – Việt, tận dụng dữ liệu đơn ngữ ngoài miền để cải thiện chất lượng dịch\n",
      "\n",
      " Câu hỏi: Theo kết quả thử nghiệm, hệ dịch Adapt_System cải thiện bao nhiêu điểm BLEU so với Baseline_G trong miền pháp lý?\n",
      "  A. 0.84  →  distance = 9.2398\n",
      "  B. 1.37  →  distance = 9.1576\n",
      "  C. 1.5  →  distance = 9.5871\n",
      "  D. 2.21  →  distance = 9.3409\n",
      "👉 Đáp án gợi ý: B - 1.37\n",
      "\n",
      " Câu hỏi: Công nghệ in bê tông 3D được thực hiện theo quy trình nào?\n",
      "  A. Gia công cắt gọt vật liệu  →  distance = 9.9495\n",
      "  B. Đúc bê tông trong ván khuôn  →  distance = 10.4267\n",
      "  C. Sản xuất bồi đắp bằng cách chồng lớp vật liệu từ mô hình CAD  →  distance = 10.9441\n",
      "  D. Ép nén bê tông truyền thống  →  distance = 10.4526\n",
      "👉 Đáp án gợi ý: A - Gia công cắt gọt vật liệu\n",
      "\n",
      " Câu hỏi: Công nghệ nào được phát triển tại Đại học Loughborough (Anh)?\n",
      "  A. Contour Crafting  →  distance = 10.4377\n",
      "  B. Concrete Printing  →  distance = 10.4895\n",
      "  C. D-Shape  →  distance = 10.4536\n",
      "  D. Emerging Objects  →  distance = 10.7569\n",
      "👉 Đáp án gợi ý: A - Contour Crafting\n",
      "\n",
      " Câu hỏi: Công nghệ in bê tông 3D nào vừa cho phép kiểm soát tốt cấu trúc sản phẩm, vừa sử dụng bê tông cốt liệu sợi tổng hợp cường độ cao?\n",
      "  A. Contour Crafting  →  distance = 10.9374\n",
      "  B. Concrete Printing  →  distance = 10.5955\n",
      "  C. Concrete On-Site 3D Printing  →  distance = 10.7387\n",
      "  D. FDM  →  distance = 10.2051\n",
      "👉 Đáp án gợi ý: D - FDM\n",
      "\n",
      " Câu hỏi: Nội dung chính của bài báo Public_004 là gì?\n",
      "  A. Trình bày ứng dụng in 3D trong y tế  →  distance = 10.8948\n",
      "  B. Giới thiệu, phân tích ưu nhược điểm các công nghệ in bê tông 3D trong xây dựng, so sánh tính khả thi áp dụng ở Việt Nam  →  distance = 10.5447\n",
      "  C. Mô tả chi tiết công thức chế tạo bê tông UHPC  →  distance = 10.6949\n",
      "  D. Giới thiệu công nghệ CAD trong xây dựng  →  distance = 11.5302\n",
      "👉 Đáp án gợi ý: B - Giới thiệu, phân tích ưu nhược điểm các công nghệ in bê tông 3D trong xây dựng, so sánh tính khả thi áp dụng ở Việt Nam\n",
      "\n",
      " Câu hỏi: Trong tài liệu, ưu điểm của công nghệ Contour Crafting so với Concrete Printing là gì?\n",
      "  A. Độ mịn bề mặt sản phẩm tốt hơn  →  distance = 8.5771\n",
      "  B. Khả năng chịu lực cao hơn  →  distance = 8.3139\n",
      "  C. Chi phí rẻ hơn  →  distance = 8.4279\n",
      "  D. Quy mô in lớn hơn  →  distance = 8.1303\n",
      "👉 Đáp án gợi ý: D - Quy mô in lớn hơn\n",
      "\n",
      " Câu hỏi: OCR là viết tắt của gì trong bài báo này?\n",
      "  A. Optical Character Recognition  →  distance = 9.5228\n",
      "  B. Optical Code Reader  →  distance = 11.1212\n",
      "  C. Optical Computing Resource  →  distance = 9.6013\n",
      "  D. Optical Capture Recognition  →  distance = 9.7568\n",
      "👉 Đáp án gợi ý: A - Optical Character Recognition\n",
      "\n",
      " Câu hỏi: Giải pháp thử nghiệm tại Học viện Ngân hàng được lựa chọn là gì, theo tài liệu Public_005?\n",
      "  A. Viettel OCR  →  distance = 9.8925\n",
      "  B. FPT.AI Reader  →  distance = 9.5216\n",
      "  C. Google Vision AI  →  distance = 9.7141\n",
      "  D. Tesseract  →  distance = 9.2248\n",
      "👉 Đáp án gợi ý: D - Tesseract\n",
      "\n",
      " Câu hỏi: Công nghệ nào vừa cho phép máy tính tự động nhận biết ký tự trên hình ảnh, vừa có khả năng đọc hiểu dữ liệu chữ viết tay và chữ in?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m chunk_embeddings = []\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc_chunks:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# 💥 FIX: Đặt max_length an toàn cho chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     emb = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphobert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    104\u001b[39m         chunk_embeddings.append(emb)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text, model, tokenizer, max_len)\u001b[39m\n\u001b[32m     19\u001b[39m inputs = tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=max_len).to(model.device)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:862\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    860\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    876\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:606\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:513\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    512\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:440\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    439\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    450\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:363\u001b[39m, in \u001b[36mRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    361\u001b[39m is_causal = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    373\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "# Thêm import cần thiết cho tính toán Euclid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Đặt giới hạn an toàn cho PhoBERT-base\n",
    "SAFE_MAX_LENGTH = 256 # Giới hạn 256 token là an toàn nhất cho PhoBERT-base\n",
    "# Giữ 514 cho phần kết hợp QA vì mô hình có thể chấp nhận nếu đã cắt ngắn, \n",
    "# nhưng 256 là an toàn hơn. Tuy nhiên, tôi sẽ giữ 514 ở đây để giữ tính đồng nhất với logic gốc.\n",
    "QA_MAX_LENGTH = 256\n",
    "\n",
    "# Hàm nhúng (đã cập nhật)\n",
    "def get_embedding(text, model, tokenizer, max_len=512):\n",
    "    # Đảm bảo tensor luôn ở device của model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    return out.last_hidden_state[:, 0, :] # CLS token\n",
    "\n",
    "# Hàm tính khoảng cách Euclid giữa một vector và nhiều vector\n",
    "def euclidean_distance(query_emb, context_embeddings):\n",
    "    # query_emb: [1, hidden_size]\n",
    "    # context_embeddings: [n_chunks, hidden_size]\n",
    "    \n",
    "    # Mở rộng kích thước của query để phù hợp với context_embeddings\n",
    "    # Thao tác này tương đương với tính (context_embeddings - query_emb).norm(p=2, dim=1)\n",
    "    \n",
    "    # torch.cdist là cách hiệu quả và trực tiếp để tính khoảng cách Euclid\n",
    "    # giữa hai tập hợp vector. Tuy nhiên, đối với một query so với nhiều context,\n",
    "    # cách tính norm sau khi trừ là đơn giản và rõ ràng hơn.\n",
    "    \n",
    "    # Mở rộng query_emb để có kích thước [n_chunks, hidden_size]\n",
    "    query_expanded = query_emb.expand_as(context_embeddings)\n",
    "    \n",
    "    # Tính hiệu số, sau đó tính chuẩn L2 (Euclidean norm) trên dimension vector (dim=1)\n",
    "    distances = torch.norm(context_embeddings - query_expanded, p=2, dim=1)\n",
    "    \n",
    "    return distances.squeeze() # Trả về tensor 1D chứa các khoảng cách\n",
    "\n",
    "# ===== 1. Load PhoBERT (Đã sửa đổi) =====\n",
    "model_name = \"vinai/phobert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "phobert = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "try: \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    phobert.to(device)\n",
    "    print(f\"Model đang chạy trên thiết bị: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cảnh báo: Không thể chuyển model sang GPU: {e}\")\n",
    "    phobert.to(\"cpu\")\n",
    "\n",
    "# ===== 3. Đọc file câu hỏi =====\n",
    "csv_path = \"training_input/question.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ===== 4. Dự đoán =====\n",
    "for idx, row in df.iterrows():\n",
    "    question = str(row[\"Question\"]).strip()\n",
    "    options = [str(row[\"A\"]), str(row[\"B\"]), str(row[\"C\"]), str(row[\"D\"])]\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    \n",
    "    # 💥 FIX: Đặt max_length an toàn cho câu hỏi\n",
    "    question_embedding = get_embedding(question, phobert, tokenizer, max_len=512)\n",
    "    \n",
    "    if question_embedding is None:\n",
    "        continue # Bỏ qua câu hỏi rỗng\n",
    "        \n",
    "    # Truy vấn DB (không thay đổi)\n",
    "    cur.execute(\"\"\"SELECT document FROM dbvector1 \n",
    "                 Order by embedding  <=> %s :: vector asc\n",
    "                 LIMIT 1;\"\"\", (question_embedding.squeeze().detach().cpu().numpy().tolist(),))\n",
    "    results = cur.fetchall()\n",
    "    x = results[0]\n",
    "    document_name = x[0]\n",
    "    print(f\"\\n Câu hỏi: {question}\")\n",
    "   \n",
    "    # Đọc và chia đoạn tài liệu (logic chia theo 256 ký tự được giữ nguyên)\n",
    "    md_path = os.path.join(\"training_output\", document_name, \"main.md\")\n",
    "    if not os.path.exists(md_path): continue\n",
    "    \n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_text = f.read()\n",
    "\n",
    "    # 🚨 CẢNH BÁO: Logic loại bỏ dấu cách (re.sub) vẫn được giữ nguyên theo yêu cầu,\n",
    "    # nhưng nó làm giảm đáng kể chất lượng kết quả nhúng.\n",
    "    processed_text = re.sub(r'\\s+', '', doc_text.strip()) \n",
    "    doc_chunks = []\n",
    "    MAX_CHARS = 300 # Giữ 500 ký tự\n",
    "    for i in range(0, len(processed_text), MAX_CHARS):\n",
    "        chunk = processed_text[i:i + MAX_CHARS]\n",
    "        doc_chunks.append(chunk)\n",
    "\n",
    "    # Tạo embedding cho từng đoạn\n",
    "    chunk_embeddings = []\n",
    "    for chunk in doc_chunks:\n",
    "        # 💥 FIX: Đặt max_length an toàn cho chunk\n",
    "        emb = get_embedding(chunk, phobert, tokenizer, max_len=300)\n",
    "        if emb is not None:\n",
    "            chunk_embeddings.append(emb)\n",
    "\n",
    "    if not chunk_embeddings:\n",
    "        print(f\"Không tìm thấy chunk hợp lệ trong tài liệu {document_name}. Bỏ qua.\")\n",
    "        continue\n",
    "\n",
    "    chunk_embeddings = torch.cat(chunk_embeddings, dim=0)  # [n_chunks, hidden_size]\n",
    "\n",
    "    distances = []\n",
    "    for i, opt in enumerate(options):\n",
    "        # *Tối ưu hóa: Nhúng chỉ tùy chọn, so sánh với tất cả các đoạn.*\n",
    "        qa_text = f\"{question} {opt}\"\n",
    "        \n",
    "        # 💥 FIX: Đặt max_length an toàn cho QA text\n",
    "        qa_emb = get_embedding(qa_text, phobert, tokenizer, max_len=512)\n",
    "\n",
    "        if qa_emb is None:\n",
    "            min_dist = float('inf')\n",
    "        else:\n",
    "            # 🔄 THAY ĐỔI LỚN: Tính khoảng cách Euclid (L2 Distance)\n",
    "            dists = euclidean_distance(qa_emb, chunk_embeddings)\n",
    "            \n",
    "            # ➡️ Tìm khoảng cách NHỎ NHẤT (vì khoảng cách Euclid: Càng nhỏ càng tốt)\n",
    "            min_dist = dists.min().item()\n",
    "            \n",
    "        distances.append(min_dist)\n",
    "        # In ra khoảng cách thay vì similarity\n",
    "        print(f\"  {labels[i]}. {opt}  →  distance = {min_dist:.4f}\")\n",
    "\n",
    "    # ➡️ THAY ĐỔI LỚN: Tìm chỉ mục có khoảng cách NHỎ NHẤT (argmin)\n",
    "    best_idx = torch.argmin(torch.tensor(distances)).item()\n",
    "    print(f\"👉 Đáp án gợi ý: {labels[best_idx]} - {options[best_idx]}\")\n",
    "    \n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac88fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f66b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
