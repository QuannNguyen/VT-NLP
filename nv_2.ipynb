{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\", # B·∫Øt bu·ªôc k·∫øt n·ªëi qua m·∫°ng TCP/IP\n",
    "    dbname=\"postgres\", \n",
    "    user=\"postgres\", \n",
    "    password=\"postgres_password\"\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b37a8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C·∫£nh b√°o: Kh√¥ng th·ªÉ chuy·ªÉn model sang GPU: CUDA error: device-side assert triggered\n",
      "Search for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      " C√¢u h·ªèi: Trong m√¥ h√¨nh nh√† th√¥ng minh, IoT ch·ªß y·∫øu ƒë√≥ng vai tr√≤ g√¨?\n",
      "  A. L∆∞u tr·ªØ d·ªØ li·ªáu tr√™n m√°y ch·ªß  ‚Üí  distance = 9.8515\n",
      "  B. K·∫øt n·ªëi Internet v√† qu·∫£n l√Ω thi·∫øt b·ªã t·ª´ xa  ‚Üí  distance = 9.5775\n",
      "  C. Cung c·∫•p d·ªãch v·ª• ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn  ‚Üí  distance = 10.6536\n",
      "  D. Thay th·∫ø ho√†n to√†n ƒëi·ªán to√°n ƒë√°m m√¢y  ‚Üí  distance = 9.8783\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - K·∫øt n·ªëi Internet v√† qu·∫£n l√Ω thi·∫øt b·ªã t·ª´ xa\n",
      "\n",
      " C√¢u h·ªèi: NƒÉm 2010, c√¥ng ngh·ªá n√†o th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ki·ªÉm so√°t ra v√†o ng√¥i nh√† th√¥ng minh?\n",
      "  A. Bluetooth Low Energy  ‚Üí  distance = 10.9916\n",
      "  B. RFID v√† nh·∫≠n d·∫°ng khu√¥n m·∫∑t/v√¢n tay  ‚Üí  distance = 9.3244\n",
      "  C. Zigbee  ‚Üí  distance = 10.1951\n",
      "  D. Blockchain  ‚Üí  distance = 10.1562\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - RFID v√† nh·∫≠n d·∫°ng khu√¥n m·∫∑t/v√¢n tay\n",
      "\n",
      " C√¢u h·ªèi: B·ªô c·∫£m bi·∫øn trong nh√† th√¥ng minh th·ª±c hi·ªán ch·ª©c nƒÉng g√¨ v√† b·ªô truy·ªÅn ƒë·ªông c√≥ vai tr√≤ g√¨?\n",
      "  A. B·ªô c·∫£m bi·∫øn hi·ªÉn th·ªã d·ªØ li·ªáu, b·ªô truy·ªÅn ƒë·ªông thu th·∫≠p d·ªØ li·ªáu  ‚Üí  distance = 8.3637\n",
      "  B. B·ªô c·∫£m bi·∫øn l∆∞u tr·ªØ d·ªØ li·ªáu, b·ªô truy·ªÅn ƒë·ªông hi·ªÉn th·ªã d·ªØ li·ªáu  ‚Üí  distance = 8.3250\n",
      "  C. B·ªô c·∫£m bi·∫øn thu th·∫≠p d·ªØ li·ªáu, b·ªô truy·ªÅn ƒë·ªông th·ª±c hi·ªán l·ªánh t·ª´ m√°y ch·ªß ho·∫∑c thi·∫øt b·ªã ƒëi·ªÅu khi·ªÉn  ‚Üí  distance = 8.4127\n",
      "  D. B·ªô c·∫£m bi·∫øn ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã, b·ªô truy·ªÅn ƒë·ªông l∆∞u tr·ªØ d·ªØ li·ªáu  ‚Üí  distance = 8.3850\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - B·ªô c·∫£m bi·∫øn l∆∞u tr·ªØ d·ªØ li·ªáu, b·ªô truy·ªÅn ƒë·ªông hi·ªÉn th·ªã d·ªØ li·ªáu\n",
      "\n",
      " C√¢u h·ªèi: N·ªôi dung ch√≠nh c·ªßa b√†i b√°o nghi√™n c·ª©u Public_001 v·ªÅ IoT trong x√¢y d·ª±ng nh√† th√¥ng minh l√† g√¨?\n",
      "  A. Gi·ªõi thi·ªáu c√°c ·ª©ng d·ª•ng c·ª• th·ªÉ c·ªßa ƒëi·ªán to√°n ƒë√°m m√¢y  ‚Üí  distance = 10.9261\n",
      "  B. Ph√¢n t√≠ch l·ª£i √≠ch v√† chi ph√≠ tri·ªÉn khai h·ªá th·ªëng c·∫£m bi·∫øn  ‚Üí  distance = 11.4108\n",
      "  C. Ph√¢n lo·∫°i v√† ƒë√°nh gi√° nghi√™n c·ª©u IoT ·ª©ng d·ª•ng nh√† th√¥ng minh trong 10 nƒÉm qua  ‚Üí  distance = 11.4527\n",
      "  D. Tr√¨nh b√†y chi ti·∫øt v·ªÅ c√¥ng ngh·ªá vi m·∫°ch trong IoT  ‚Üí  distance = 11.5481\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: A - Gi·ªõi thi·ªáu c√°c ·ª©ng d·ª•ng c·ª• th·ªÉ c·ªßa ƒëi·ªán to√°n ƒë√°m m√¢y\n",
      "\n",
      " C√¢u h·ªèi: D·ª±a v√†o th√¥ng tin trong t√†i li·ªáu Public_001, h√£y t√≠nh s·ªë b√†i b√°o trung b√¨nh m·ªói l·ªõp trong nghi√™n c·ª©u ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p.\n",
      "  A. 45-50  ‚Üí  distance = 10.3552\n",
      "  B. 55-60  ‚Üí  distance = 10.3300\n",
      "  C. 70-80  ‚Üí  distance = 11.9169\n",
      "  D. 90-100  ‚Üí  distance = 10.1353\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - 90-100\n",
      "\n",
      " C√¢u h·ªèi: Theo t√†i li·ªáu Public_002, h·ªçc ph·∫ßn B·∫£o m·∫≠t Web trang b·ªã ki·∫øn th·ª©c v·ªÅ b·ªô r·ªßi ro ph·ªï bi·∫øn n√†o?\n",
      "  A. OWASP Top 10  ‚Üí  distance = 9.0374\n",
      "  B. IEEE 802.11  ‚Üí  distance = 9.4525\n",
      "  C. ISO/IEC 27001  ‚Üí  distance = 8.8918\n",
      "  D. NIST Cybersecurity Framework  ‚Üí  distance = 8.8534\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - NIST Cybersecurity Framework\n",
      "\n",
      " C√¢u h·ªèi: D·ª±a tr√™n t√†i li·ªáu Public_002, m√¥ t·∫£ h·ªçc ph·∫ßn C∆° s·ªü d·ªØ li·ªáu n√¢ng cao, h√£y li·ªát k√™ c√°c lo·∫°i CSDL kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong n·ªôi dung h·ªçc ph·∫ßn n√†y.\n",
      "  A. CSDL NoSQL  ‚Üí  distance = 10.0470\n",
      "  B. CSDL h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng  ‚Üí  distance = 9.6025\n",
      "  C. CSDL b√°n c·∫•u tr√∫c XML  ‚Üí  distance = 9.6373\n",
      "  D. CSDL quan h·ªá truy·ªÅn th·ªëng  ‚Üí  distance = 9.5590\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - CSDL quan h·ªá truy·ªÅn th·ªëng\n",
      "\n",
      " C√¢u h·ªèi: N√™u chi ti·∫øt n·ªôi dung ƒë√†o t·∫°o v·ªÅ b·∫£o m·∫≠t web v√† b·∫£o m·∫≠t c∆° s·ªü d·ªØ li·ªáu trong t√†i li·ªáu Public_002.\n",
      "  A. OWASP Top 10 / CSDL NoSQL  ‚Üí  distance = 10.8811\n",
      "  B. IEEE 802.11 / CSDL h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng  ‚Üí  distance = 10.9088\n",
      "  C. DSA / CSDL b√°n c·∫•u tr√∫c XML  ‚Üí  distance = 10.9699\n",
      "  D. To√°n cao c·∫•p / CSDL quan h·ªá truy·ªÅn th·ªëng  ‚Üí  distance = 11.4299\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: A - OWASP Top 10 / CSDL NoSQL\n",
      "\n",
      " C√¢u h·ªèi: Nh√≥m m√¥n h·ªçc trong t√†i li·ªáu Public_002 t·∫≠p trung cung c·∫•p cho sinh vi√™n ki·∫øn th·ª©c t·ªïng quan v·ªÅ lƒ©nh v·ª±c n√†o?\n",
      "  A. C∆° kh√≠ ‚Äì ƒêi·ªán t·ª≠  ‚Üí  distance = 9.6892\n",
      "  B. C√¥ng ngh·ªá th√¥ng tin (CSDL, m·∫°ng, b·∫£o m·∫≠t, d·ªØ li·ªáu l·ªõn, IoT, h·ªçc m√°y)  ‚Üí  distance = 7.0313\n",
      "  C. Kinh t·∫ø ‚Äì Qu·∫£n l√Ω  ‚Üí  distance = 9.4850\n",
      "  D. Ng√¥n ng·ªØ h·ªçc  ‚Üí  distance = 9.7276\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - C√¥ng ngh·ªá th√¥ng tin (CSDL, m·∫°ng, b·∫£o m·∫≠t, d·ªØ li·ªáu l·ªõn, IoT, h·ªçc m√°y)\n",
      "\n",
      " C√¢u h·ªèi: Trong t√†i li·ªáu Public_002, ƒë·ªëi v·ªõi ƒëo·∫°n li·ªát k√™ c√°c h·ªçc ph·∫ßn t·ª´ s·ªë 47 ƒë·∫øn 81, n·∫øu c√°c h·ªçc ph·∫ßn n√†y ƒë∆∞·ª£c tr·∫£i ƒë·ªÅu tr√™n 7 trang, trung b√¨nh m·ªói trang m√¥ t·∫£ bao nhi√™u h·ªçc ph·∫ßn?\n",
      "  A. 5  ‚Üí  distance = 10.3733\n",
      "  B. 6  ‚Üí  distance = 10.5861\n",
      "  C. 7  ‚Üí  distance = 10.0191\n",
      "  D. 8  ‚Üí  distance = 10.0928\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: C - 7\n",
      "\n",
      " C√¢u h·ªèi: ƒêi·ªÉm BLEU ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng b·∫£n d·ªãch trong t√†i li·ªáu Public_003 l√† g√¨?\n",
      "  A. M·ªôt thu·∫≠t to√°n d·ªãch m√°y  ‚Üí  distance = 10.2217\n",
      "  B. M·ªôt lo·∫°i d·ªØ li·ªáu song ng·ªØ  ‚Üí  distance = 9.9553\n",
      "  C. M·ªôt ph∆∞∆°ng ph√°p ƒë√°nh gi√° t·ª± ƒë·ªông ch·∫•t l∆∞·ª£ng d·ªãch m√°y  ‚Üí  distance = 10.1526\n",
      "  D. M·ªôt h·ªá d·ªãch m√£ ngu·ªìn m·ªü  ‚Üí  distance = 10.3071\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - M·ªôt lo·∫°i d·ªØ li·ªáu song ng·ªØ\n",
      "\n",
      " C√¢u h·ªèi: C√¥ng c·ª• n√†o ƒë∆∞·ª£c nh√≥m t√°c gi·∫£ c·ªßa t√†i li·ªáu Public_003 s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán c√°c h·ªá d·ªãch n∆° ron?\n",
      "  A. Moses  ‚Üí  distance = 9.7865\n",
      "  B. OpenNMT  ‚Üí  distance = 11.0836\n",
      "  C. TensorFlow  ‚Üí  distance = 9.3547\n",
      "  D. Fairseq  ‚Üí  distance = 9.2159\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - Fairseq\n",
      "\n",
      " C√¢u h·ªèi: ƒêi·ªÉm BLEU v√† c√¥ng c·ª• hu·∫•n luy·ªán h·ªá d·ªãch n∆° ron trong nghi√™n c·ª©u n√†y l√† g√¨?\n",
      "  A. M·ªôt thu·∫≠t to√°n d·ªãch m√°y / Moses  ‚Üí  distance = 9.9506\n",
      "  B. M·ªôt lo·∫°i d·ªØ li·ªáu song ng·ªØ / OpenNMT  ‚Üí  distance = 9.8316\n",
      "  C. M·ªôt ph∆∞∆°ng ph√°p ƒë√°nh gi√° t·ª± ƒë·ªông ch·∫•t l∆∞·ª£ng d·ªãch m√°y / TensorFlow  ‚Üí  distance = 9.2654\n",
      "  D. M·ªôt h·ªá d·ªãch m√£ ngu·ªìn m·ªü / Fairseq  ‚Üí  distance = 9.5456\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: C - M·ªôt ph∆∞∆°ng ph√°p ƒë√°nh gi√° t·ª± ƒë·ªông ch·∫•t l∆∞·ª£ng d·ªãch m√°y / TensorFlow\n",
      "\n",
      " C√¢u h·ªèi: N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu Public_003 l√† g√¨?\n",
      "  A. So s√°nh d·ªãch m√°y th·ªëng k√™ v√† d·ªãch m√°y v√≠ d·ª•  ‚Üí  distance = 10.1350\n",
      "  B. ƒê·ªÅ xu·∫•t ph∆∞∆°ng ph√°p th√≠ch ·ª©ng mi·ªÅn m·ªõi cho d·ªãch m√°y n∆° ron Anh ‚Äì Vi·ªát, t·∫≠n d·ª•ng d·ªØ li·ªáu ƒë∆°n ng·ªØ ngo√†i mi·ªÅn ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªãch  ‚Üí  distance = 9.9033\n",
      "  C. Ph√¢n t√≠ch d·ªØ li·ªáu song ng·ªØ trong lƒ©nh v·ª±c ph√°p l√Ω  ‚Üí  distance = 9.9673\n",
      "  D. Tr√¨nh b√†y l√Ω thuy·∫øt v·ªÅ RNN hai chi·ªÅu  ‚Üí  distance = 10.0892\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - ƒê·ªÅ xu·∫•t ph∆∞∆°ng ph√°p th√≠ch ·ª©ng mi·ªÅn m·ªõi cho d·ªãch m√°y n∆° ron Anh ‚Äì Vi·ªát, t·∫≠n d·ª•ng d·ªØ li·ªáu ƒë∆°n ng·ªØ ngo√†i mi·ªÅn ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªãch\n",
      "\n",
      " C√¢u h·ªèi: Theo k·∫øt qu·∫£ th·ª≠ nghi·ªám, h·ªá d·ªãch Adapt_System c·∫£i thi·ªán bao nhi√™u ƒëi·ªÉm BLEU so v·ªõi Baseline_G trong mi·ªÅn ph√°p l√Ω?\n",
      "  A. 0.84  ‚Üí  distance = 9.2398\n",
      "  B. 1.37  ‚Üí  distance = 9.1576\n",
      "  C. 1.5  ‚Üí  distance = 9.5871\n",
      "  D. 2.21  ‚Üí  distance = 9.3409\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - 1.37\n",
      "\n",
      " C√¢u h·ªèi: C√¥ng ngh·ªá in b√™ t√¥ng 3D ƒë∆∞·ª£c th·ª±c hi·ªán theo quy tr√¨nh n√†o?\n",
      "  A. Gia c√¥ng c·∫Øt g·ªçt v·∫≠t li·ªáu  ‚Üí  distance = 9.9495\n",
      "  B. ƒê√∫c b√™ t√¥ng trong v√°n khu√¥n  ‚Üí  distance = 10.4267\n",
      "  C. S·∫£n xu·∫•t b·ªìi ƒë·∫Øp b·∫±ng c√°ch ch·ªìng l·ªõp v·∫≠t li·ªáu t·ª´ m√¥ h√¨nh CAD  ‚Üí  distance = 10.9441\n",
      "  D. √âp n√©n b√™ t√¥ng truy·ªÅn th·ªëng  ‚Üí  distance = 10.4526\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: A - Gia c√¥ng c·∫Øt g·ªçt v·∫≠t li·ªáu\n",
      "\n",
      " C√¢u h·ªèi: C√¥ng ngh·ªá n√†o ƒë∆∞·ª£c ph√°t tri·ªÉn t·∫°i ƒê·∫°i h·ªçc Loughborough (Anh)?\n",
      "  A. Contour Crafting  ‚Üí  distance = 10.4377\n",
      "  B. Concrete Printing  ‚Üí  distance = 10.4895\n",
      "  C. D-Shape  ‚Üí  distance = 10.4536\n",
      "  D. Emerging Objects  ‚Üí  distance = 10.7569\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: A - Contour Crafting\n",
      "\n",
      " C√¢u h·ªèi: C√¥ng ngh·ªá in b√™ t√¥ng 3D n√†o v·ª´a cho ph√©p ki·ªÉm so√°t t·ªët c·∫•u tr√∫c s·∫£n ph·∫©m, v·ª´a s·ª≠ d·ª•ng b√™ t√¥ng c·ªët li·ªáu s·ª£i t·ªïng h·ª£p c∆∞·ªùng ƒë·ªô cao?\n",
      "  A. Contour Crafting  ‚Üí  distance = 10.9374\n",
      "  B. Concrete Printing  ‚Üí  distance = 10.5955\n",
      "  C. Concrete On-Site 3D Printing  ‚Üí  distance = 10.7387\n",
      "  D. FDM  ‚Üí  distance = 10.2051\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - FDM\n",
      "\n",
      " C√¢u h·ªèi: N·ªôi dung ch√≠nh c·ªßa b√†i b√°o Public_004 l√† g√¨?\n",
      "  A. Tr√¨nh b√†y ·ª©ng d·ª•ng in 3D trong y t·∫ø  ‚Üí  distance = 10.8948\n",
      "  B. Gi·ªõi thi·ªáu, ph√¢n t√≠ch ∆∞u nh∆∞·ª£c ƒëi·ªÉm c√°c c√¥ng ngh·ªá in b√™ t√¥ng 3D trong x√¢y d·ª±ng, so s√°nh t√≠nh kh·∫£ thi √°p d·ª•ng ·ªü Vi·ªát Nam  ‚Üí  distance = 10.5447\n",
      "  C. M√¥ t·∫£ chi ti·∫øt c√¥ng th·ª©c ch·∫ø t·∫°o b√™ t√¥ng UHPC  ‚Üí  distance = 10.6949\n",
      "  D. Gi·ªõi thi·ªáu c√¥ng ngh·ªá CAD trong x√¢y d·ª±ng  ‚Üí  distance = 11.5302\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: B - Gi·ªõi thi·ªáu, ph√¢n t√≠ch ∆∞u nh∆∞·ª£c ƒëi·ªÉm c√°c c√¥ng ngh·ªá in b√™ t√¥ng 3D trong x√¢y d·ª±ng, so s√°nh t√≠nh kh·∫£ thi √°p d·ª•ng ·ªü Vi·ªát Nam\n",
      "\n",
      " C√¢u h·ªèi: Trong t√†i li·ªáu, ∆∞u ƒëi·ªÉm c·ªßa c√¥ng ngh·ªá Contour Crafting so v·ªõi Concrete Printing l√† g√¨?\n",
      "  A. ƒê·ªô m·ªãn b·ªÅ m·∫∑t s·∫£n ph·∫©m t·ªët h∆°n  ‚Üí  distance = 8.5771\n",
      "  B. Kh·∫£ nƒÉng ch·ªãu l·ª±c cao h∆°n  ‚Üí  distance = 8.3139\n",
      "  C. Chi ph√≠ r·∫ª h∆°n  ‚Üí  distance = 8.4279\n",
      "  D. Quy m√¥ in l·ªõn h∆°n  ‚Üí  distance = 8.1303\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - Quy m√¥ in l·ªõn h∆°n\n",
      "\n",
      " C√¢u h·ªèi: OCR l√† vi·∫øt t·∫Øt c·ªßa g√¨ trong b√†i b√°o n√†y?\n",
      "  A. Optical Character Recognition  ‚Üí  distance = 9.5228\n",
      "  B. Optical Code Reader  ‚Üí  distance = 11.1212\n",
      "  C. Optical Computing Resource  ‚Üí  distance = 9.6013\n",
      "  D. Optical Capture Recognition  ‚Üí  distance = 9.7568\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: A - Optical Character Recognition\n",
      "\n",
      " C√¢u h·ªèi: Gi·∫£i ph√°p th·ª≠ nghi·ªám t·∫°i H·ªçc vi·ªán Ng√¢n h√†ng ƒë∆∞·ª£c l·ª±a ch·ªçn l√† g√¨, theo t√†i li·ªáu Public_005?\n",
      "  A. Viettel OCR  ‚Üí  distance = 9.8925\n",
      "  B. FPT.AI Reader  ‚Üí  distance = 9.5216\n",
      "  C. Google Vision AI  ‚Üí  distance = 9.7141\n",
      "  D. Tesseract  ‚Üí  distance = 9.2248\n",
      "üëâ ƒê√°p √°n g·ª£i √Ω: D - Tesseract\n",
      "\n",
      " C√¢u h·ªèi: C√¥ng ngh·ªá n√†o v·ª´a cho ph√©p m√°y t√≠nh t·ª± ƒë·ªông nh·∫≠n bi·∫øt k√Ω t·ª± tr√™n h√¨nh ·∫£nh, v·ª´a c√≥ kh·∫£ nƒÉng ƒë·ªçc hi·ªÉu d·ªØ li·ªáu ch·ªØ vi·∫øt tay v√† ch·ªØ in?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m chunk_embeddings = []\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc_chunks:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# üí• FIX: ƒê·∫∑t max_length an to√†n cho chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     emb = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphobert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    104\u001b[39m         chunk_embeddings.append(emb)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text, model, tokenizer, max_len)\u001b[39m\n\u001b[32m     19\u001b[39m inputs = tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=max_len).to(model.device)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:862\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    860\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    876\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:606\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:513\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    512\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:440\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    439\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    450\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py312env/lib64/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:363\u001b[39m, in \u001b[36mRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    361\u001b[39m is_causal = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    373\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "# Th√™m import c·∫ßn thi·∫øt cho t√≠nh to√°n Euclid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ƒê·∫∑t gi·ªõi h·∫°n an to√†n cho PhoBERT-base\n",
    "SAFE_MAX_LENGTH = 256 # Gi·ªõi h·∫°n 256 token l√† an to√†n nh·∫•t cho PhoBERT-base\n",
    "# Gi·ªØ 514 cho ph·∫ßn k·∫øt h·ª£p QA v√¨ m√¥ h√¨nh c√≥ th·ªÉ ch·∫•p nh·∫≠n n·∫øu ƒë√£ c·∫Øt ng·∫Øn, \n",
    "# nh∆∞ng 256 l√† an to√†n h∆°n. Tuy nhi√™n, t√¥i s·∫Ω gi·ªØ 514 ·ªü ƒë√¢y ƒë·ªÉ gi·ªØ t√≠nh ƒë·ªìng nh·∫•t v·ªõi logic g·ªëc.\n",
    "QA_MAX_LENGTH = 256\n",
    "\n",
    "# H√†m nh√∫ng (ƒë√£ c·∫≠p nh·∫≠t)\n",
    "def get_embedding(text, model, tokenizer, max_len=512):\n",
    "    # ƒê·∫£m b·∫£o tensor lu√¥n ·ªü device c·ªßa model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    return out.last_hidden_state[:, 0, :] # CLS token\n",
    "\n",
    "# H√†m t√≠nh kho·∫£ng c√°ch Euclid gi·ªØa m·ªôt vector v√† nhi·ªÅu vector\n",
    "def euclidean_distance(query_emb, context_embeddings):\n",
    "    # query_emb: [1, hidden_size]\n",
    "    # context_embeddings: [n_chunks, hidden_size]\n",
    "    \n",
    "    # M·ªü r·ªông k√≠ch th∆∞·ªõc c·ªßa query ƒë·ªÉ ph√π h·ª£p v·ªõi context_embeddings\n",
    "    # Thao t√°c n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi t√≠nh (context_embeddings - query_emb).norm(p=2, dim=1)\n",
    "    \n",
    "    # torch.cdist l√† c√°ch hi·ªáu qu·∫£ v√† tr·ª±c ti·∫øp ƒë·ªÉ t√≠nh kho·∫£ng c√°ch Euclid\n",
    "    # gi·ªØa hai t·∫≠p h·ª£p vector. Tuy nhi√™n, ƒë·ªëi v·ªõi m·ªôt query so v·ªõi nhi·ªÅu context,\n",
    "    # c√°ch t√≠nh norm sau khi tr·ª´ l√† ƒë∆°n gi·∫£n v√† r√µ r√†ng h∆°n.\n",
    "    \n",
    "    # M·ªü r·ªông query_emb ƒë·ªÉ c√≥ k√≠ch th∆∞·ªõc [n_chunks, hidden_size]\n",
    "    query_expanded = query_emb.expand_as(context_embeddings)\n",
    "    \n",
    "    # T√≠nh hi·ªáu s·ªë, sau ƒë√≥ t√≠nh chu·∫©n L2 (Euclidean norm) tr√™n dimension vector (dim=1)\n",
    "    distances = torch.norm(context_embeddings - query_expanded, p=2, dim=1)\n",
    "    \n",
    "    return distances.squeeze() # Tr·∫£ v·ªÅ tensor 1D ch·ª©a c√°c kho·∫£ng c√°ch\n",
    "\n",
    "# ===== 1. Load PhoBERT (ƒê√£ s·ª≠a ƒë·ªïi) =====\n",
    "model_name = \"vinai/phobert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "phobert = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "try: \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    phobert.to(device)\n",
    "    print(f\"Model ƒëang ch·∫°y tr√™n thi·∫øt b·ªã: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"C·∫£nh b√°o: Kh√¥ng th·ªÉ chuy·ªÉn model sang GPU: {e}\")\n",
    "    phobert.to(\"cpu\")\n",
    "\n",
    "# ===== 3. ƒê·ªçc file c√¢u h·ªèi =====\n",
    "csv_path = \"training_input/question.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ===== 4. D·ª± ƒëo√°n =====\n",
    "for idx, row in df.iterrows():\n",
    "    question = str(row[\"Question\"]).strip()\n",
    "    options = [str(row[\"A\"]), str(row[\"B\"]), str(row[\"C\"]), str(row[\"D\"])]\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    \n",
    "    # üí• FIX: ƒê·∫∑t max_length an to√†n cho c√¢u h·ªèi\n",
    "    question_embedding = get_embedding(question, phobert, tokenizer, max_len=512)\n",
    "    \n",
    "    if question_embedding is None:\n",
    "        continue # B·ªè qua c√¢u h·ªèi r·ªóng\n",
    "        \n",
    "    # Truy v·∫•n DB (kh√¥ng thay ƒë·ªïi)\n",
    "    cur.execute(\"\"\"SELECT document FROM dbvector1 \n",
    "                 Order by embedding  <=> %s :: vector asc\n",
    "                 LIMIT 1;\"\"\", (question_embedding.squeeze().detach().cpu().numpy().tolist(),))\n",
    "    results = cur.fetchall()\n",
    "    x = results[0]\n",
    "    document_name = x[0]\n",
    "    print(f\"\\n C√¢u h·ªèi: {question}\")\n",
    "   \n",
    "    # ƒê·ªçc v√† chia ƒëo·∫°n t√†i li·ªáu (logic chia theo 256 k√Ω t·ª± ƒë∆∞·ª£c gi·ªØ nguy√™n)\n",
    "    md_path = os.path.join(\"training_output\", document_name, \"main.md\")\n",
    "    if not os.path.exists(md_path): continue\n",
    "    \n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_text = f.read()\n",
    "\n",
    "    # üö® C·∫¢NH B√ÅO: Logic lo·∫°i b·ªè d·∫•u c√°ch (re.sub) v·∫´n ƒë∆∞·ª£c gi·ªØ nguy√™n theo y√™u c·∫ßu,\n",
    "    # nh∆∞ng n√≥ l√†m gi·∫£m ƒë√°ng k·ªÉ ch·∫•t l∆∞·ª£ng k·∫øt qu·∫£ nh√∫ng.\n",
    "    processed_text = re.sub(r'\\s+', '', doc_text.strip()) \n",
    "    doc_chunks = []\n",
    "    MAX_CHARS = 300 # Gi·ªØ 500 k√Ω t·ª±\n",
    "    for i in range(0, len(processed_text), MAX_CHARS):\n",
    "        chunk = processed_text[i:i + MAX_CHARS]\n",
    "        doc_chunks.append(chunk)\n",
    "\n",
    "    # T·∫°o embedding cho t·ª´ng ƒëo·∫°n\n",
    "    chunk_embeddings = []\n",
    "    for chunk in doc_chunks:\n",
    "        # üí• FIX: ƒê·∫∑t max_length an to√†n cho chunk\n",
    "        emb = get_embedding(chunk, phobert, tokenizer, max_len=300)\n",
    "        if emb is not None:\n",
    "            chunk_embeddings.append(emb)\n",
    "\n",
    "    if not chunk_embeddings:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y chunk h·ª£p l·ªá trong t√†i li·ªáu {document_name}. B·ªè qua.\")\n",
    "        continue\n",
    "\n",
    "    chunk_embeddings = torch.cat(chunk_embeddings, dim=0)  # [n_chunks, hidden_size]\n",
    "\n",
    "    distances = []\n",
    "    for i, opt in enumerate(options):\n",
    "        # *T·ªëi ∆∞u h√≥a: Nh√∫ng ch·ªâ t√πy ch·ªçn, so s√°nh v·ªõi t·∫•t c·∫£ c√°c ƒëo·∫°n.*\n",
    "        qa_text = f\"{question} {opt}\"\n",
    "        \n",
    "        # üí• FIX: ƒê·∫∑t max_length an to√†n cho QA text\n",
    "        qa_emb = get_embedding(qa_text, phobert, tokenizer, max_len=512)\n",
    "\n",
    "        if qa_emb is None:\n",
    "            min_dist = float('inf')\n",
    "        else:\n",
    "            # üîÑ THAY ƒê·ªîI L·ªöN: T√≠nh kho·∫£ng c√°ch Euclid (L2 Distance)\n",
    "            dists = euclidean_distance(qa_emb, chunk_embeddings)\n",
    "            \n",
    "            # ‚û°Ô∏è T√¨m kho·∫£ng c√°ch NH·ªé NH·∫§T (v√¨ kho·∫£ng c√°ch Euclid: C√†ng nh·ªè c√†ng t·ªët)\n",
    "            min_dist = dists.min().item()\n",
    "            \n",
    "        distances.append(min_dist)\n",
    "        # In ra kho·∫£ng c√°ch thay v√¨ similarity\n",
    "        print(f\"  {labels[i]}. {opt}  ‚Üí  distance = {min_dist:.4f}\")\n",
    "\n",
    "    # ‚û°Ô∏è THAY ƒê·ªîI L·ªöN: T√¨m ch·ªâ m·ª•c c√≥ kho·∫£ng c√°ch NH·ªé NH·∫§T (argmin)\n",
    "    best_idx = torch.argmin(torch.tensor(distances)).item()\n",
    "    print(f\"üëâ ƒê√°p √°n g·ª£i √Ω: {labels[best_idx]} - {options[best_idx]}\")\n",
    "    \n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac88fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f66b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
